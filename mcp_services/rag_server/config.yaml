retrieval:
  top_k: 30  # Reduceret fra 50 for hurtigere søgning
  rerank_top_k: 5  # Reduceret fra 10 for hurtigere reranking
  similarity_threshold: 0.70  # Sænket for at få flere relevante resultater
  strategy: "hybrid"

embedding:
  provider: "sentence-transformers"
  model_name: "all-MiniLM-L6-v2"  # Beholder denne model da den er hurtig og effektiv
  vector_size: 384
  batch_size: 32  # Tilføjet for hurtigere embedding-generering
  normalize_embeddings: true  # Tilføjet for bedre søgeresultater

query_classifier:
  enabled: true
  threshold: 0.75  # Justeret for bedre balance
  model_path: null  # Use default classifier

llm:
  provider: "ollama"
  model_name: "llama3.1:8b"
  base_url: "http://localhost:11434"
  temperature: 0.1  # Reduceret for mere deterministiske svar
  max_tokens: 1000  # Reduceret for hurtigere svar
  num_ctx: 2048  # Reduceret kontekstvindue for hurtigere inferens
  num_thread: 4  # Optimeret for M1 Mac
  repeat_penalty: 1.1  # Tilføjet for at undgå gentagelser
  top_k: 30  # Tilføjet for hurtigere token-generering
  top_p: 0.9  # Tilføjet for hurtigere token-generering

indexing:
  chunk_size: 400  # Reduceret for mindre chunks
  chunk_overlap: 30  # Reduceret for mindre overlap
  ignored_dirs:
    - "node_modules"
    - "dist"
    - "build"
    - ".git"
    - ".turbo"
    - ".yarn"
    - "__pycache__"
    - ".next"
    - "mcp-venv"  # Tilføjet for at undgå indeksering af virtual environment
    - "venv"
    - "logs"
    - "chroma_db"  # Undgå rekursiv indeksering
  supported_extensions:
    - ".py"
    - ".js"
    - ".jsx"
    - ".ts"
    - ".tsx"

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
  file_path: "logs/rag_server.log"

performance:
  cache_embeddings: true  # Tilføjet for at cache embeddings
  preload_models: true  # Tilføjet for hurtigere startup
  optimize_for_m1: true  # Tilføjet for M1 Mac optimering