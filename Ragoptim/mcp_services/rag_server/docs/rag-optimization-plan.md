# RAG Optimeringsplan: Trin-for-trin guide

Dette dokument indeholder en detaljeret plan for at optimere RAG-implementeringen i LearningLab-projektet. Planen er baseret p√• forskningsvaliderede teknikker og er designet til at give betydelige ydeevneforbedringer.

## Fase 1: Grundl√¶ggende optimering (allerede implementeret)

- [x] Streaming-implementering for Ollama
- [x] Optimerede Ollama-parametre (temperatur, kontekstvindue, thread-count)
- [x] Prompt-optimering
- [x] Optimering af vector search
- [x] Forbedret server-h√•ndtering

## Fase 2: Avanceret optimering

### Trin 1: Implementer Approximate Caching (Proximity) ‚úÖ

**Form√•l:** Reducere latenstid ved at genbruge tidligere s√∏geresultater for lignende foresp√∏rgsler.

**Status:** ‚úÖ Implementeret

**Implementeringsdetaljer:**

1. Oprettet en ny fil `proximity_cache.py` med f√∏lgende funktionalitet:
   - Caching af s√∏geresultater baseret p√• kosinus-lighed mellem foresp√∏rgsler
   - Konfigurerbar t√¶rskel for lighed (default: 0.8)
   - Konfigurerbar maksimal cache-st√∏rrelse (default: 100)
   - Automatisk fjernelse af √¶ldste cache-indgange n√•r maksimal st√∏rrelse n√•s

2. Integreret cachen i `vector_search_server_fastapi.py`:
   - Cache tjekkes f√∏r normal vektors√∏gning
   - Resultater gemmes i cachen efter hver s√∏gning
   - Statistik om cache-brug logges

3. Tilf√∏jet en ny endpoint `/cache/stats` til at vise cache-statistik:
   - Antal indgange i cachen
   - Maksimal cache-st√∏rrelse
   - T√¶rskel for lighed
   - Hit rate

**Forventet ydeevneforbedring:** 40-60% reduktion i latenstid for gentagne eller lignende foresp√∏rgsler.

### Trin 2: Optim√©r chunking-strategien ‚úÖ

**Form√•l:** Forbedre relevansen af s√∏geresultater og reducere antallet af chunks.

**Status:** ‚úÖ Implementeret

**Implementeringsdetaljer:**

1. Oprettet en separat `semantic_chunking.py` modul med f√∏lgende funktionalitet:
   - Semantisk opdeling af tekst baseret p√• embeddings-lighed mellem linjer
   - GPU-acceleration hvis tilg√¶ngelig
   - Konfigurerbare parametre for minimum og maksimum chunk-st√∏rrelse
   - Automatisk normalisering af embeddings for pr√¶cis lighed

2. Integreret semantisk chunking i `index_code_chunks.py`:
   - Erstattet den tidligere chunking-logik med semantisk chunking
   - Optimeret parametre (min_size=300, max_size=600) for bedre balance mellem kontekst og pr√¶cision

**Forventet ydeevneforbedring:** 20-30% reduktion i antallet af chunks og forbedret relevans af s√∏geresultater.

### Trin 3: Skift til FastAPI for bedre ydeevne ‚úÖ

**Form√•l:** Forbedre serverens gennemstr√∏mning og h√•ndtering af samtidige foresp√∏rgsler.

**Status:** ‚úÖ Implementeret

**Implementeringsdetaljer:**

1. Oprettet en ny `vector_search_server_fastapi.py` med f√∏lgende forbedringer:
   - Skiftet fra Flask til FastAPI for bedre ydeevne
   - Implementeret asynkrone endpoints for bedre h√•ndtering af samtidige foresp√∏rgsler
   - Tilf√∏jet Pydantic-modeller for validering af input
   - Forbedret fejlh√•ndtering med strukturerede HTTP-fejlkoder

2. Oprettet et nyt startscript `start-rag-server-fastapi.sh` med f√∏lgende optimering:
   - Konfigureret Uvicorn med flere worker-processer (4)
   - Automatisk installation af n√∏dvendige afh√¶ngigheder
   - Forbedret logning og fejlh√•ndtering
   - Automatisk rydning af Python-cache for at sikre friske imports

**Forventet ydeevneforbedring:** 50-100% forbedring i gennemstr√∏mning under belastning.

### Trin 4: Implementer GPU-acceleration (hvis tilg√¶ngelig) ‚úÖ

**Form√•l:** Accelerer embedding-generering og reranking ved hj√¶lp af GPU.

**Status:** ‚úÖ Implementeret

**Implementeringsdetaljer:**

1. Tilf√∏jet GPU-detektion og -support i alle relevante komponenter:
   - `vector_search_server_fastapi.py` bruger GPU til embedding-generering hvis tilg√¶ngelig
   - `semantic_chunking.py` bruger GPU til at beregne embeddings for chunking
   - `index_code_chunks.py` bruger GPU til at generere embeddings under indeksering

2. Implementeret automatisk fallback til CPU hvis GPU ikke er tilg√¶ngelig:
   ```python
   device = "cuda" if torch.cuda.is_available() else "cpu"
   print(f"üñ•Ô∏è Bruger {device} til embedding-generering")
   ```

3. Optimeret batch-st√∏rrelse for bedre GPU-udnyttelse:
   - St√∏rre batch-st√∏rrelse (32) for chunking
   - Tilpasset batch-st√∏rrelse for embedding-generering

**Forventet ydeevneforbedring:** 5-10x hurtigere embedding-generering og reranking (hvis GPU er tilg√¶ngelig).

## Fase 3: Avancerede optimeringer (fremtidige trin)

### Trin 5: Skift til FAISS for hurtigere vektors√∏gning

**Form√•l:** Forbedre s√∏gehastigheden ved at bruge FAISS i stedet for ChromaDB.

**Implementering:**

1. Installer FAISS:
   ```bash
   pip install faiss-cpu  # eller faiss-gpu hvis GPU er tilg√¶ngelig
   ```

2. Eksporter embeddings fra ChromaDB til FAISS:
   ```python
   import faiss
   import numpy as np
   import json
   import os
   
   def export_chroma_to_faiss(chroma_dir, output_dir):
       """
       Eksporter embeddings fra ChromaDB til FAISS.
       
       Args:
           chroma_dir: Sti til ChromaDB-mappen
           output_dir: Sti til output-mappen
       """
       # Initialiser ChromaDB
       client = chromadb.Client()
       collection = client.get_collection("code_chunks", persist_directory=chroma_dir)
       
       # Hent alle embeddings
       results = collection.get(include=["embeddings", "documents", "metadatas"])
       
       embeddings = results["embeddings"]
       documents = results["documents"]
       metadatas = results["metadatas"]
       
       # Konverter embeddings til numpy array
       embeddings_np = np.array(embeddings).astype('float32')
       
       # Opret FAISS-index
       dimension = embeddings_np.shape[1]
       index = faiss.IndexFlatIP(dimension)  # Indre produkt for kosinus-lighed
       
       # Tilf√∏j embeddings til index
       index.add(embeddings_np)
       
       # Gem index
       os.makedirs(output_dir, exist_ok=True)
       faiss.write_index(index, os.path.join(output_dir, "faiss_index.bin"))
       
       # Gem metadata og dokumenter
       with open(os.path.join(output_dir, "metadata.json"), "w") as f:
           json.dump({"documents": documents, "metadatas": metadatas}, f)
       
       print(f"‚úÖ Eksporteret {len(embeddings)} embeddings til FAISS-index")
   ```

3. Opdater `vector_search_server.py` til at bruge FAISS:
   ```python
   import faiss
   import json
   
   # Indl√¶s FAISS-index
   index = faiss.read_index("faiss_index.bin")
   
   # Indl√¶s metadata og dokumenter
   with open("metadata.json", "r") as f:
       metadata_store = json.load(f)
   
   @app.post("/search")
   async def search(request: SearchRequest):
       # ... eksisterende kode ...
       
       # Lav embedding for foresp√∏rgslen
       query_embedding = model.encode(query_text, batch_size=1, show_progress_bar=False)
       
       # Tjek cache f√∏rst
       # ... eksisterende kode ...
       
       # Hvis ikke i cache, s√∏g med FAISS
       query_embedding_np = np.array([query_embedding]).astype('float32')
       distances, indices = index.search(query_embedding_np, n_results * 2)
       
       # Konverter resultater til samme format som ChromaDB
       results = []
       for i, idx in enumerate(indices[0]):
           if idx < 0 or idx >= len(metadata_store["documents"]):
               continue
           
           results.append({
               "chunk": metadata_store["documents"][idx],
               "metadata": metadata_store["metadatas"][idx],
               "distance": float(distances[0][i])
           })
       
       # ... resten af koden ...
   ```

### Trin 6: Implementer mikrotjeneste-arkitektur

**Form√•l:** Forbedre skalerbarhed og fejltolerance ved at opdele systemet i mindre, specialiserede tjenester.

**Implementering:**

Dette trin kr√¶ver en mere omfattende omstrukturering og vil blive beskrevet i en separat plan, hvis de tidligere optimeringer ikke giver tilstr√¶kkelig ydeevneforbedring.

## Testplan

For hver optimering b√∏r f√∏lgende tests udf√∏res:

1. **Ydeevnetest**:
   ```bash
   python test-rag-performance.py
   ```

2. **Funktionalitetstest**:
   ```bash
   python test-rag-pipeline.py "hvordan fungerer authentication?"
   ```

3. **Belastningstest** (valgfri):
   ```bash
   # Installer k6
   brew install k6
   
   # Opret en k6-testfil
   cat > load-test.js << EOF
   import http from 'k6/http';
   import { sleep } from 'k6';
   
   export default function () {
     const payload = JSON.stringify({
       query: 'hvordan fungerer authentication?',
       n_results: 3
     });
     
     const params = {
       headers: {
         'Content-Type': 'application/json',
       },
     };
     
     http.post('http://localhost:5004/search', payload, params);
     sleep(1);
   }
   EOF
   
   # K√∏r belastningstest
   k6 run --vus 10 --duration 30s load-test.js
   ```

## Forventede resultater

Ved implementering af alle optimeringer i Fase 2 forventes f√∏lgende forbedringer:

1. **Approximate Caching**: 40-60% reduktion i latenstid for gentagne eller lignende foresp√∏rgsler.
2. **Semantisk chunking**: 20-30% reduktion i antallet af chunks og forbedret relevans af s√∏geresultater.
3. **FastAPI**: 50-100% forbedring i gennemstr√∏mning under belastning.
4. **GPU-acceleration**: 5-10x hurtigere embedding-generering og reranking (hvis GPU er tilg√¶ngelig).

Ved implementering af alle optimeringer i Fase 3 forventes yderligere forbedringer:

1. **FAISS**: 2-4x hurtigere vektors√∏gning sammenlignet med ChromaDB.
2. **Mikrotjeneste-arkitektur**: Forbedret skalerbarhed og fejltolerance.

## Status og konklusion

### Aktuel status

- **Fase 1: Grundl√¶ggende optimering** ‚úÖ Fuldt implementeret
- **Fase 2: Avanceret optimering** ‚úÖ Fuldt implementeret
  - Trin 1: Approximate Caching (Proximity) ‚úÖ
  - Trin 2: Optimeret chunking-strategi ‚úÖ
  - Trin 3: Skift til FastAPI ‚úÖ
  - Trin 4: GPU-acceleration ‚úÖ
- **Fase 3: Avancerede optimeringer** ‚è≥ Planlagt til fremtidig implementering
  - Trin 5: Skift til FAISS ‚è≥
  - Trin 6: Mikrotjeneste-arkitektur ‚è≥

### N√¶ste skridt

Alle planlagte optimeringer i Fase 1 og Fase 2 er nu implementeret. Baseret p√• ydeevnetests og brugerfeedback kan vi vurdere, om det er n√∏dvendigt at implementere de mere avancerede optimeringer i Fase 3.

F√∏r vi g√•r videre med Fase 3, anbefales det at:

1. Udf√∏re grundige ydeevnetests for at m√•le forbedringerne fra Fase 2
2. Indsamle brugerfeedback om systemets responsivitet og relevans
3. Identificere eventuelle flaskehalse, der stadig eksisterer

Hvis ydeevnen stadig ikke er tilfredsstillende efter Fase 2, kan vi begynde at implementere FAISS for hurtigere vektors√∏gning som det n√¶ste skridt.